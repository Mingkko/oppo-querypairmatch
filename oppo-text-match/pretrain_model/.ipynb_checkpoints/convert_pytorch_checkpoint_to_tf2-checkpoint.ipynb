{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "from . import (\n",
    "    ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    BART_PRETRAINED_MODEL_ARCHIVE_LIST,\n",
    "    BERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    DPR_CONTEXT_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST,\n",
    "    DPR_QUESTION_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST,\n",
    "    DPR_READER_PRETRAINED_MODEL_ARCHIVE_LIST,\n",
    "    ELECTRA_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    LXMERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    T5_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    WEIGHTS_NAME,\n",
    "    XLM_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    AlbertConfig,\n",
    "    BartConfig,\n",
    "    BertConfig,\n",
    "    CamembertConfig,\n",
    "    CTRLConfig,\n",
    "    DistilBertConfig,\n",
    "    DPRConfig,\n",
    "    ElectraConfig,\n",
    "    FlaubertConfig,\n",
    "    GPT2Config,\n",
    "    LxmertConfig,\n",
    "    OpenAIGPTConfig,\n",
    "    RobertaConfig,\n",
    "    T5Config,\n",
    "    TFAlbertForPreTraining,\n",
    "    TFBartForConditionalGeneration,\n",
    "    TFBertForPreTraining,\n",
    "    TFBertForQuestionAnswering,\n",
    "    TFBertForSequenceClassification,\n",
    "    TFCamembertForMaskedLM,\n",
    "    TFCTRLLMHeadModel,\n",
    "    TFDistilBertForMaskedLM,\n",
    "    TFDistilBertForQuestionAnswering,\n",
    "    TFDPRContextEncoder,\n",
    "    TFDPRQuestionEncoder,\n",
    "    TFDPRReader,\n",
    "    TFElectraForPreTraining,\n",
    "    TFFlaubertWithLMHeadModel,\n",
    "    TFGPT2LMHeadModel,\n",
    "    TFLxmertForPreTraining,\n",
    "    TFLxmertVisualFeatureEncoder,\n",
    "    TFOpenAIGPTLMHeadModel,\n",
    "    TFRobertaForMaskedLM,\n",
    "    TFRobertaForSequenceClassification,\n",
    "    TFT5ForConditionalGeneration,\n",
    "    TFTransfoXLLMHeadModel,\n",
    "    TFXLMRobertaForMaskedLM,\n",
    "    TFXLMWithLMHeadModel,\n",
    "    TFXLNetLMHeadModel,\n",
    "    TransfoXLConfig,\n",
    "    XLMConfig,\n",
    "    XLMRobertaConfig,\n",
    "    XLNetConfig,\n",
    "    cached_path,\n",
    "    is_torch_available,\n",
    "    load_pytorch_checkpoint_in_tf2_model,\n",
    ")\n",
    "from .file_utils import hf_bucket_url\n",
    "from .utils import logging\n",
    "\n",
    "\n",
    "if is_torch_available():\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    from . import (\n",
    "        AlbertForPreTraining,\n",
    "        BartForConditionalGeneration,\n",
    "        BertForPreTraining,\n",
    "        BertForQuestionAnswering,\n",
    "        BertForSequenceClassification,\n",
    "        CamembertForMaskedLM,\n",
    "        CTRLLMHeadModel,\n",
    "        DistilBertForMaskedLM,\n",
    "        DistilBertForQuestionAnswering,\n",
    "        DPRContextEncoder,\n",
    "        DPRQuestionEncoder,\n",
    "        DPRReader,\n",
    "        ElectraForPreTraining,\n",
    "        FlaubertWithLMHeadModel,\n",
    "        GPT2LMHeadModel,\n",
    "        LxmertForPreTraining,\n",
    "        LxmertVisualFeatureEncoder,\n",
    "        OpenAIGPTLMHeadModel,\n",
    "        RobertaForMaskedLM,\n",
    "        RobertaForSequenceClassification,\n",
    "        T5ForConditionalGeneration,\n",
    "        TransfoXLLMHeadModel,\n",
    "        XLMRobertaForMaskedLM,\n",
    "        XLMWithLMHeadModel,\n",
    "        XLNetLMHeadModel,\n",
    "    )\n",
    "\n",
    "\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"bart\": (\n",
    "        BartConfig,\n",
    "        TFBartForConditionalGeneration,\n",
    "        BartForConditionalGeneration,\n",
    "        BART_PRETRAINED_MODEL_ARCHIVE_LIST,\n",
    "    ),\n",
    "    \"bert\": (\n",
    "        BertConfig,\n",
    "        TFBertForPreTraining,\n",
    "        BertForPreTraining,\n",
    "        BERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\": (\n",
    "        BertConfig,\n",
    "        TFBertForQuestionAnswering,\n",
    "        BertForQuestionAnswering,\n",
    "        BERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\": (\n",
    "        BertConfig,\n",
    "        TFBertForQuestionAnswering,\n",
    "        BertForQuestionAnswering,\n",
    "        BERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"bert-base-cased-finetuned-mrpc\": (\n",
    "        BertConfig,\n",
    "        TFBertForSequenceClassification,\n",
    "        BertForSequenceClassification,\n",
    "        BERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"dpr\": (\n",
    "        DPRConfig,\n",
    "        TFDPRQuestionEncoder,\n",
    "        TFDPRContextEncoder,\n",
    "        TFDPRReader,\n",
    "        DPRQuestionEncoder,\n",
    "        DPRContextEncoder,\n",
    "        DPRReader,\n",
    "        DPR_CONTEXT_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST,\n",
    "        DPR_QUESTION_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST,\n",
    "        DPR_READER_PRETRAINED_MODEL_ARCHIVE_LIST,\n",
    "    ),\n",
    "    \"gpt2\": (\n",
    "        GPT2Config,\n",
    "        TFGPT2LMHeadModel,\n",
    "        GPT2LMHeadModel,\n",
    "        GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"xlnet\": (\n",
    "        XLNetConfig,\n",
    "        TFXLNetLMHeadModel,\n",
    "        XLNetLMHeadModel,\n",
    "        XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"xlm\": (\n",
    "        XLMConfig,\n",
    "        TFXLMWithLMHeadModel,\n",
    "        XLMWithLMHeadModel,\n",
    "        XLM_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"xlm-roberta\": (\n",
    "        XLMRobertaConfig,\n",
    "        TFXLMRobertaForMaskedLM,\n",
    "        XLMRobertaForMaskedLM,\n",
    "        XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"transfo-xl\": (\n",
    "        TransfoXLConfig,\n",
    "        TFTransfoXLLMHeadModel,\n",
    "        TransfoXLLMHeadModel,\n",
    "        TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"openai-gpt\": (\n",
    "        OpenAIGPTConfig,\n",
    "        TFOpenAIGPTLMHeadModel,\n",
    "        OpenAIGPTLMHeadModel,\n",
    "        OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"roberta\": (\n",
    "        RobertaConfig,\n",
    "        TFRobertaForMaskedLM,\n",
    "        RobertaForMaskedLM,\n",
    "        ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"roberta-large-mnli\": (\n",
    "        RobertaConfig,\n",
    "        TFRobertaForSequenceClassification,\n",
    "        RobertaForSequenceClassification,\n",
    "        ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"camembert\": (\n",
    "        CamembertConfig,\n",
    "        TFCamembertForMaskedLM,\n",
    "        CamembertForMaskedLM,\n",
    "        CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"flaubert\": (\n",
    "        FlaubertConfig,\n",
    "        TFFlaubertWithLMHeadModel,\n",
    "        FlaubertWithLMHeadModel,\n",
    "        FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"distilbert\": (\n",
    "        DistilBertConfig,\n",
    "        TFDistilBertForMaskedLM,\n",
    "        DistilBertForMaskedLM,\n",
    "        DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"distilbert-base-distilled-squad\": (\n",
    "        DistilBertConfig,\n",
    "        TFDistilBertForQuestionAnswering,\n",
    "        DistilBertForQuestionAnswering,\n",
    "        DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"lxmert\": (\n",
    "        LxmertConfig,\n",
    "        TFLxmertForPreTraining,\n",
    "        LxmertForPreTraining,\n",
    "        LXMERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"lxmert-visual-feature-encoder\": (\n",
    "        LxmertConfig,\n",
    "        TFLxmertVisualFeatureEncoder,\n",
    "        LxmertVisualFeatureEncoder,\n",
    "        LXMERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"ctrl\": (\n",
    "        CTRLConfig,\n",
    "        TFCTRLLMHeadModel,\n",
    "        CTRLLMHeadModel,\n",
    "        CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"albert\": (\n",
    "        AlbertConfig,\n",
    "        TFAlbertForPreTraining,\n",
    "        AlbertForPreTraining,\n",
    "        ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"t5\": (\n",
    "        T5Config,\n",
    "        TFT5ForConditionalGeneration,\n",
    "        T5ForConditionalGeneration,\n",
    "        T5_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "    \"electra\": (\n",
    "        ElectraConfig,\n",
    "        TFElectraForPreTraining,\n",
    "        ElectraForPreTraining,\n",
    "        ELECTRA_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def convert_pt_checkpoint_to_tf(\n",
    "    model_type, pytorch_checkpoint_path, config_file, tf_dump_path, compare_with_pt_model=False, use_cached_models=True\n",
    "):\n",
    "    if model_type not in MODEL_CLASSES:\n",
    "        raise ValueError(\"Unrecognized model type, should be one of {}.\".format(list(MODEL_CLASSES.keys())))\n",
    "\n",
    "    config_class, model_class, pt_model_class, aws_config_map = MODEL_CLASSES[model_type]\n",
    "\n",
    "    # Initialise TF model\n",
    "    if config_file in aws_config_map:\n",
    "        config_file = cached_path(aws_config_map[config_file], force_download=not use_cached_models)\n",
    "    config = config_class.from_json_file(config_file)\n",
    "    config.output_hidden_states = True\n",
    "    config.output_attentions = True\n",
    "    print(\"Building TensorFlow model from configuration: {}\".format(str(config)))\n",
    "    tf_model = model_class(config)\n",
    "\n",
    "    # Load weights from tf checkpoint\n",
    "    if pytorch_checkpoint_path in aws_config_map.keys():\n",
    "        pytorch_checkpoint_url = hf_bucket_url(pytorch_checkpoint_path, filename=WEIGHTS_NAME)\n",
    "        pytorch_checkpoint_path = cached_path(pytorch_checkpoint_url, force_download=not use_cached_models)\n",
    "    # Load PyTorch checkpoint in tf2 model:\n",
    "    tf_model = load_pytorch_checkpoint_in_tf2_model(tf_model, pytorch_checkpoint_path)\n",
    "\n",
    "    if compare_with_pt_model:\n",
    "        tfo = tf_model(tf_model.dummy_inputs, training=False)  # build the network\n",
    "\n",
    "        state_dict = torch.load(pytorch_checkpoint_path, map_location=\"cpu\")\n",
    "        pt_model = pt_model_class.from_pretrained(\n",
    "            pretrained_model_name_or_path=None, config=config, state_dict=state_dict\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pto = pt_model(**pt_model.dummy_inputs)\n",
    "\n",
    "        np_pt = pto[0].numpy()\n",
    "        np_tf = tfo[0].numpy()\n",
    "        diff = np.amax(np.abs(np_pt - np_tf))\n",
    "        print(\"Max absolute difference between models outputs {}\".format(diff))\n",
    "        assert diff <= 2e-2, \"Error, model absolute difference is >2e-2: {}\".format(diff)\n",
    "\n",
    "    # Save pytorch-model\n",
    "    print(\"Save TensorFlow model to {}\".format(tf_dump_path))\n",
    "    tf_model.save_weights(tf_dump_path, save_format=\"h5\")\n",
    "\n",
    "\n",
    "def convert_all_pt_checkpoints_to_tf(\n",
    "    args_model_type,\n",
    "    tf_dump_path,\n",
    "    model_shortcut_names_or_path=None,\n",
    "    config_shortcut_names_or_path=None,\n",
    "    compare_with_pt_model=False,\n",
    "    use_cached_models=False,\n",
    "    remove_cached_files=False,\n",
    "    only_convert_finetuned_models=False,\n",
    "):\n",
    "\n",
    "    if args_model_type is None:\n",
    "        model_types = list(MODEL_CLASSES.keys())\n",
    "    else:\n",
    "        model_types = [args_model_type]\n",
    "\n",
    "    for j, model_type in enumerate(model_types, start=1):\n",
    "        print(\"=\" * 100)\n",
    "        print(\" Converting model type {}/{}: {}\".format(j, len(model_types), model_type))\n",
    "        print(\"=\" * 100)\n",
    "        if model_type not in MODEL_CLASSES:\n",
    "            raise ValueError(\n",
    "                \"Unrecognized model type {}, should be one of {}.\".format(model_type, list(MODEL_CLASSES.keys()))\n",
    "            )\n",
    "\n",
    "        config_class, model_class, pt_model_class, aws_model_maps, aws_config_map = MODEL_CLASSES[model_type]\n",
    "\n",
    "        if model_shortcut_names_or_path is None:\n",
    "            model_shortcut_names_or_path = list(aws_model_maps.keys())\n",
    "        if config_shortcut_names_or_path is None:\n",
    "            config_shortcut_names_or_path = model_shortcut_names_or_path\n",
    "\n",
    "        for i, (model_shortcut_name, config_shortcut_name) in enumerate(\n",
    "            zip(model_shortcut_names_or_path, config_shortcut_names_or_path), start=1\n",
    "        ):\n",
    "            print(\"-\" * 100)\n",
    "            if \"-squad\" in model_shortcut_name or \"-mrpc\" in model_shortcut_name or \"-mnli\" in model_shortcut_name:\n",
    "                if not only_convert_finetuned_models:\n",
    "                    print(\"    Skipping finetuned checkpoint {}\".format(model_shortcut_name))\n",
    "                    continue\n",
    "                model_type = model_shortcut_name\n",
    "            elif only_convert_finetuned_models:\n",
    "                print(\"    Skipping not finetuned checkpoint {}\".format(model_shortcut_name))\n",
    "                continue\n",
    "            print(\n",
    "                \"    Converting checkpoint {}/{}: {} - model_type {}\".format(\n",
    "                    i, len(aws_config_map), model_shortcut_name, model_type\n",
    "                )\n",
    "            )\n",
    "            print(\"-\" * 100)\n",
    "\n",
    "            if config_shortcut_name in aws_config_map:\n",
    "                config_file = cached_path(aws_config_map[config_shortcut_name], force_download=not use_cached_models)\n",
    "            else:\n",
    "                config_file = cached_path(config_shortcut_name, force_download=not use_cached_models)\n",
    "\n",
    "            if model_shortcut_name in aws_model_maps:\n",
    "                model_file = cached_path(aws_model_maps[model_shortcut_name], force_download=not use_cached_models)\n",
    "            else:\n",
    "                model_file = cached_path(model_shortcut_name, force_download=not use_cached_models)\n",
    "\n",
    "            if os.path.isfile(model_shortcut_name):\n",
    "                model_shortcut_name = \"converted_model\"\n",
    "\n",
    "            convert_pt_checkpoint_to_tf(\n",
    "                model_type=model_type,\n",
    "                pytorch_checkpoint_path=model_file,\n",
    "                config_file=config_file,\n",
    "                tf_dump_path=os.path.join(tf_dump_path, model_shortcut_name + \"-tf_model.h5\"),\n",
    "                compare_with_pt_model=compare_with_pt_model,\n",
    "            )\n",
    "            if remove_cached_files:\n",
    "                os.remove(config_file)\n",
    "                os.remove(model_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Required parameters\n",
    "    parser.add_argument(\n",
    "        \"--tf_dump_path\", default=None, type=str, required=True, help=\"Path to the output Tensorflow dump file.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_type\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Model type selected in the list of {}. If not given, will download and convert all the models from AWS.\".format(\n",
    "            list(MODEL_CLASSES.keys())\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pytorch_checkpoint_path\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Path to the PyTorch checkpoint path or shortcut name to download from AWS. \"\n",
    "        \"If not given, will download and convert all the checkpoints from AWS.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config_file\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"The config json file corresponding to the pre-trained model. \\n\"\n",
    "        \"This specifies the model architecture. If not given and \"\n",
    "        \"--pytorch_checkpoint_path is not given or is a shortcut name\"\n",
    "        \"use the configuration associated to the shortcut name on the AWS\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--compare_with_pt_model\", action=\"store_true\", help=\"Compare Tensorflow and PyTorch model predictions.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_cached_models\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Use cached models if possible instead of updating to latest checkpoint versions.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--remove_cached_files\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Remove pytorch models after conversion (save memory when converting in batches).\",\n",
    "    )\n",
    "    parser.add_argument(\"--only_convert_finetuned_models\", action=\"store_true\", help=\"Only convert finetuned models.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # if args.pytorch_checkpoint_path is not None:\n",
    "    #     convert_pt_checkpoint_to_tf(args.model_type.lower(),\n",
    "    #                                 args.pytorch_checkpoint_path,\n",
    "    #                                 args.config_file if args.config_file is not None else args.pytorch_checkpoint_path,\n",
    "    #                                 args.tf_dump_path,\n",
    "    #                                 compare_with_pt_model=args.compare_with_pt_model,\n",
    "    #                                 use_cached_models=args.use_cached_models)\n",
    "    # else:\n",
    "    convert_all_pt_checkpoints_to_tf(\n",
    "        args.model_type.lower() if args.model_type is not None else None,\n",
    "        args.tf_dump_path,\n",
    "        model_shortcut_names_or_path=[args.pytorch_checkpoint_path]\n",
    "        if args.pytorch_checkpoint_path is not None\n",
    "        else None,\n",
    "        config_shortcut_names_or_path=[args.config_file] if args.config_file is not None else None,\n",
    "        compare_with_pt_model=args.compare_with_pt_model,\n",
    "        use_cached_models=args.use_cached_models,\n",
    "        remove_cached_files=args.remove_cached_files,\n",
    "        only_convert_finetuned_models=args.only_convert_finetuned_models,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
